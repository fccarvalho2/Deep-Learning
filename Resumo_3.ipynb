{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb470a27",
   "metadata": {
    "id": "bb470a27"
   },
   "source": [
    "<a id=\"Topo\"></a><b><p style=\"text-align:center;font-size:24px\">Algoritmos de Aprendizado Profundo </p> </b> \n",
    "<p style=\"text-align:center;font-size:6px\"></p>\n",
    "<b><p style=\"text-align:center;font-size:16px\"> Resumo - Módulo 3 </p> </b>\n",
    "\n",
    "---\n",
    "\n",
    "<b><p style=\"text-align:center;font-size:24px\"> Índice </p> </b> \n",
    "\n",
    "**[I - Aprendizado não supervisionado](#Apns)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. Introdução](#Intro)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1.1. ](#)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.Métodos Clássicos](#Metodos)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.1. PCA](#PCA)**    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.2. Variantes](#Variantes)**    \n",
    "\n",
    "**[II - Autoencoders](#AutoEncoders)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. Encoder](#Encoders)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1.1. ](#)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2. Decoder](#Decoders)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[3. Denoising Autoencoders](#Denoising)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[4. Exemplos](#ExemplosAE)**   \n",
    "\n",
    "**[III - Variational Autoencoders](#VAE)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. ](#)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2. ](#)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.1. ](#)**      \n",
    "\n",
    "\n",
    "**[IV - GANs](#GANs)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. Definições](#Def)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2. GANs mais recentes](#Recentes)**  \n",
    "\n",
    "\n",
    "**[Lista de exercícios 1](#Lista1)**  \n",
    "**[Lista de exercícios 2](#Lista2)**  \n",
    "**[Lista de exercícios 3](#Lista3)**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2f01c",
   "metadata": {},
   "source": [
    "https://www.amazon.com.br/Learning-Python-Second-Fran%C3%A7ois-Chollet/dp/1617296864/ref=asc_df_1617296864/?tag=googleshopp00-20&linkCode=df0&hvadid=379787788238&hvpos=&hvnetw=g&hvrand=8481696284822609082&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9101079&hvtargid=pla-1214669039719&psc=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc1cd35",
   "metadata": {
    "id": "2cc1cd35"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.ndimage.filters import convolve\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712e4da",
   "metadata": {},
   "source": [
    "# <a id=\"Apns\"></a> I - Aprendizado não-supervisionado \n",
    "\n",
    "Quando temos situações de aprendizado supervisionado, temos os rótulos, que são a resposta\n",
    "\n",
    "O foco da classificação supervisionada é entender como a variável resposta (y, ou label) responde coma s mudanças em g(x)\n",
    "\n",
    "\n",
    "Na classificação não-supervisionada, existe um grupo grande de variável.\n",
    "Não existe um conjunto de features às quais elas respondem.\n",
    "\n",
    "Queremos descobrir estruturas não-obvias nos dados.\n",
    "\n",
    "Tarefas:\n",
    "* Redução de dimensionalidade e visualização dos dados\n",
    "\n",
    "* Descoberta de clusters / grupos / classe -> Não existem rótulos para classe\n",
    "\n",
    "* Descoberta de estrutura (probabilistic graphical models, Bayesian networks)\n",
    "\n",
    "* Modelos generativos (GANs)\n",
    "\n",
    "<b><p style=\"text-align:right\"> [Retornar ao topo](#Topo) </p> </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920305dd",
   "metadata": {},
   "source": [
    "# <a id=\"Metodos\"></a> 2. Métodos Clássicos\n",
    "\n",
    "Alguns desafios de alta dimensão:\n",
    "    \n",
    "A maldição da dimensionalidade\n",
    "Dados de alta dimensão constumam viver na fronteira do espaço amostral\n",
    "Essa maldição não ocorre em dimensões menores\n",
    "DAdos aleatórios com disstribuição uniforme no quadrado [-1,1] x [-1,1]\n",
    "Dados num círculo  ocupam piR² = 81% do quadrado\n",
    "\n",
    "\n",
    "\n",
    "Quando aumentamos para 3 dimensões, uma esfera circunscrita no cubo de lado 2 passa a ocupar 51% do volume\n",
    "\n",
    "Aumentando a dimensão, temos que dados uniformes em $[-1, 1]^d$, temos que uma esfera S d-dimensional, centrada em (0,0,0,0...,0) e de raio 1\n",
    "\n",
    "$$\\mathbb{P}(Y\\in S) \\sim \\bigg(\\frac{1}{2}\\bigg)^d$$\n",
    "\n",
    "Esses cálculos acima assumem que os dados estão uniformemente distribuídos num hipercubo.\n",
    "\n",
    "Temos também a **bênção da não-uniformidade**, que significa que os dados às vezes estão concentrados em \"variedades diferenciáveis\" (differentiable manifolds), que são equivalentes a superfícies \n",
    "\n",
    "\n",
    "## <a id=\"PCA\"></a> 2.1 Principal Component Analysis (PCA)\n",
    "\n",
    "[Excelente explicação de PCA - em inglês](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)  \n",
    "\n",
    "A análise de componentes principais é uma das técnicas de redução de dimensionalidade mais usadas na estatística. O objetivo desta técnica é obter os eixos sobre os quais a variância de um conjunto de dados multivariado é máxima. Isso pode ser bem exemplificado pela imagem abaixo.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/lNHqt.gif\" alt=\"PCA. Fonte:stack_exchange\" width=\"800\"/>\n",
    "\n",
    "Observe que a variância do conjunto é máxima quando a reta se alinha às marcas rosas, e nesta posição ele é o equivalente ao **primeiro componente principal**. Esta direção da reta é dada pelo autovetor correspondente ao maior autovalor da matriz de covariância.\n",
    "\n",
    "Os autovalores são diretamente proporcionais ao percentual da variância do conjunto em uma determinada direção (autovetor correspondente). Por este motivo, os autovalores normalizados (divididos pela soma, para somarem 1) são geralmente tratados como os indicadores de percentual da covariância explicada.\n",
    "\n",
    "Por ter a capacidade de identificar as direções onde a variância do conjunto de dados é máxima, o PCA pode ser utilizado como um método de redução de dimensionalidade, pois uma vez identificados os $x$ primeiros componentes principais, podemos selecioná-los, eliminando o demais enquanto preservamos boa parte da variância nos dados. Esta técnica é útil quando trabalhamos com dados de alta dimensionalidade, ou que possuem correlações implícitas.\n",
    "\n",
    "No *scikit-learn*, a classe PCA possui o método pca.explained_variance_, que retorna os autovalores ordenados. O plot de componente x autovalor é chamado de Scree-plot. Ele pode ser muito útil quando desejamos realizar um agrupamento não supervisionado, e não sabemos ao certo quantas classes utilizar. Outra utilidade é no caso em que desejamos reduzir a dimensionalidade dos nossos dados, mas não temos certeza de quantos componentes principais utilizar. Esta técnica é conhecida como _elbow-method_. \n",
    "\n",
    "<b><p style=\"text-align:right\"> [Retornar ao topo](#Topo) </p> </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73788b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular autovetores e autovalores, a dicaé usar o numpy.linalg.eig\n",
    "# que calcula ambos ao mesmo tempo.\n",
    "# Para PCA, usar o scikit-learn\n",
    "\n",
    "# No código abaixo, pca é uma variável referente à classe PCA treinada no conjunto de dados\n",
    "componentes = np.arange(pca.n_components_) + 1 # Soma 1 só para começar de 1\n",
    "plt.plot(componentes, pca.explained_variance_, 'o-', color = 'tab:blue', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Componente principal')\n",
    "plt.ylabel('Autovalor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9845a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef54123",
   "metadata": {},
   "source": [
    "## 2. Autoencoders\n",
    "\n",
    "Autoencoders = Redes neurais treinadas com o objetivo de copiar o seu input para o seu output.\n",
    "\n",
    "Isto é, a entrada é aproximadamente igual à saída\n",
    "\n",
    "A regularização do autoencoder é uma regularização um tanto diferente da tradicional. Ela não é aplicada diretamente no peso das redes, mas na saída intermediária (Z). Isso ajuda a fazer com que alguns dos pesos da rede sejam zero, reduzindo.\n",
    "\n",
    "Denoising autoencoders tem a ideia de evitar que uma rede muito complexa possa aprender a função identididade.\n",
    "\n",
    "\n",
    "Ruidos estruturados são úteis em situações onde o ruído esperado já é conhecido, a exemplo de marcas d'água.\n",
    "\n",
    "\n",
    "<b><p style=\"text-align:right\"> [Retornar ao topo](#Topo) </p> </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec5a06",
   "metadata": {},
   "source": [
    "https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d9ae1",
   "metadata": {},
   "source": [
    "Autoencoders para textos usam redes recorrentes. Será que dá para usar.\n",
    "\n",
    "Reproduzir slides do Steven Flores. A representação latente dos autoencoders pode ser utilizada para clusterização, porque teoricamente ele produzirá uma "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83306a",
   "metadata": {},
   "source": [
    "# RETIRADO DOS NOTEBOOKS DE AULA\n",
    "\n",
    "Todo AutoEncoder (AE) é composto de uma arquitetura Encoder-Decoder, assim como algumas redes para segmentação semântica que vimos na semana passada (i.e. [U-Nets](https://arxiv.org/pdf/1505.04597.pdf) ou [SegNets](https://arxiv.org/pdf/1511.00561.pdf)). AEs tradicionais não eram treinados usando backpropagation, mas sim com o método de **Stacking** de pares de camadas, caracterizando uma estratégia **gulosa** que ignorava interdependências entre pesos de diferentes camadas. O treinamento dessas redes era feito usando **Stacking** porque o algoritmo de backpropagation que vimos ao longo do curso ainda não era estável o bastante na época que os AEs surgiram na literatura, não conseguindo propagar os erros em redes com mais camadas escondidas. AEs atuais são treinados usando backpropagation assim como as outras arquiteturas modernas.\n",
    "\n",
    "![Linear AE](https://www.dropbox.com/s/vdfhfz6jldmidsj/Linear_AE.png?dl=1)\n",
    "\n",
    "O Encoder em um AE tradicional é composto de camadas lineares que diminuem a dimensionalidade dos dados progressivamente, criando uma região de bottleneck no meio da codificação.\n",
    "\n",
    "![Linear AE Encoder](https://www.dropbox.com/s/fyozex4grkim3ze/Linear_AE_Encoder.png?dl=1)\n",
    "\n",
    "Já as camadas do Decoder num AE recuperam gradualmente a dimensionalidade dos dados, reconstruíndo-o da forma mais fiel possível.\n",
    "\n",
    "![Linear AE Decoder](https://www.dropbox.com/s/t67b2z8bh68ei1q/Linear_AE_Decoder.png?dl=1)\n",
    "\n",
    "Ao se usar AEs Lineares em imagens, faz-se necessário linearizar os dados antes de enviá-los para as camadas Fully Connected da rede, lembrando sempre de recuperar as dimensões da imagem decodificada no fim da rede. Tanto a linearização quanto a recuperação da resolução espacial podem ser feitas usando o método *.view()* dos tensores.\n",
    "\n",
    "Percebe-se que os dados passam de uma dimensionalidade alta na entrada para uma dimensionalidade consideravelmente baixa no bottleneck da rede. Portanto AEs podem ser vistos como métodos de redução de dimensionalidade -- assim como o [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) ou o [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) -- porém treináveis para atuarem em um domínio específico (no nosso caso, em reconstrução dígitos). Assim como outros métodos de redução de dimensionalidade, a intuição dos AEs é que as informações importantes para a reconstrução dos dados sejam codificadas no bottleneck enquanto as informações não importantes sejam ignoradas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae37242",
   "metadata": {},
   "source": [
    "# Usando a representação de bottleneck como Deep Features\n",
    "\n",
    "A camada de bottleneck de um AE Linear ou Convolucional pode ser linearizada e utilizada como **Deep Features** que podem servir de entrada para um algoritmo que vá realizar uma tarefa qualquer de Machine Learning (i.e. clusterização, regressão, classificação, etc). Dessa forma, o Encoder da rede passa a ser um extrator de **Deep Features** não-supervisionado.\n",
    "\n",
    "![Conv AE Extractor](https://www.dropbox.com/s/11s5a5ynlu4g4wp/Conv_AE_Extractor.png?dl=1)\n",
    "\n",
    "Nota-se que o Encoder foi otimizado para minimizar a reconstrução dos dados -- e não a classificação dos dígitos, no caso do MNIST -- conseguindo resultados de classificação que não podem ser equiparados aos modelos treinados de forma supervisionada end-to-end, porém ainda melhores que a maioria dos **Handcrafted Features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee742e",
   "metadata": {},
   "source": [
    "# Denoising AutoEncoder\n",
    "\n",
    "Tanto AEs Lineares quanto Convolucionais podem ser adaptados para tarefas diferentes da de redução de dimensionalidade e reconstrução de imagens. Ao adicionar ruído artificial às imagens de entrada antes de passá-las para a forward, por exemplo, é possível treinar um AE para aprender a ignorar esse ruído e reconstruir as imagens originais, efetivamente realizando uma filtragem de ruído treinável. Em suma, é isso que um Denoising AutoEncoder (DAE) faz, como mostra o esquema abaixo.\n",
    "\n",
    "![Denoising AE](https://www.dropbox.com/s/mekbapirkdvwkhx/Denoising_AE.png?dl=1)\n",
    "\n",
    "É perceptível que ainda é preciso ter amostras limpas de ruído para treinar esse tipo de método, tornando-o limitado em alguns cenários. Porém, utilizando o conhecimento que se tem sobre as distribuições de ruídos em diferentes tipos de imagens (i.e. [gaussiano](https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise), [salt-and-pepper](https://en.wikipedia.org/wiki/Salt-and-pepper_noise), [ruído quântico](https://www.sciencedirect.com/topics/chemistry/quantum-noise), etc) e realizando algumas suposições sobre a natureza das imagens, é possível construir com pequenas alterações no AE tradicional um removedor de ruído específico para os dados de um certo domínio.\n",
    "\n",
    "O ruído salt-and-pepper é determinado pixel-a-pixel por uma distribuição de Bernoulli. Ou seja, dados $M$ pixels ${A_{0}, A_{1}, ..., A_{M-1}}$ de uma imagem $A$ e uma probabilidade $p$, cada pixel $A_{i}$ possui uma probabilidade $\\frac{p}{2}$ de ser setado para 0 e uma probabilidade $\\frac{p}{2}$ de ser setado para 1. No caso do ruído aditivo gaussiano, valores de uma imagem de ruído $B$ (tal que $B_{i} \\sim N(0, std)$) amostrada aleatoriamente são somados a todos os pixels da imagem $A$, resultando numa imagem $C = A + B$.\n",
    "\n",
    "Métodos mais modernos (i.e. [Noise2Noise](https://arxiv.org/pdf/1803.04189.pdf)) conseguem convergir sem a necessidade de dados não afetados pelos ruídos, tornando-os altamente aplicáveis em áreas como [imagens biomédicas](http://www.sprawls.org/ppmi2/NOISE/) ou [sensoriamento remoto](https://en.wikipedia.org/wiki/Synthetic-aperture_radar), as quais normalmente não possuem amostras \"limpas\" para treinamento de um DAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f28ee",
   "metadata": {},
   "source": [
    "# AutoEncoder Esparso\n",
    "\n",
    "[Regularizações](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2) são componentes comuns em vários métodos da área de Machine Learning que podem servir como um viés para que o algoritmo dê preferência a soluções mais simples, potencialmente prevenindo overfitting no caso de modelos **overcomplete** e/ou **small data**. A loss de um Sparse AE (SAE) adiciona um termo de regularização $\\mathcal{L}_{s}$ à loss de regressão $\\mathcal{L}_{r}$ de um AE tradicional. Dessa forma, a loss total $\\mathcal{L}_{t}$ é dada por:\n",
    "\n",
    "$\\mathcal{L}_{t}(x, \\hat{x}, z) = \\mathcal{L}_{r}(x, \\hat{x}) + \\lambda_{s} \\mathcal{L}_{s}(z).$\n",
    "\n",
    "Um AE tradicional produz features com ativações consideravelmente densas dos inputs passados a ele, já que, como o objetivo principal é reconstrução, toda informação possível deve ser mantida nas representações latentes da rede.\n",
    "\n",
    "![Dense AE](https://www.dropbox.com/s/nfiix8cfk5g9wue/Sparse_AE_1.png?dl=1)\n",
    "\n",
    "Em contraponto, SAEs produzem representações esparsas dos dados que podem ser utilizadas para realizar [**Sparse Coding**](https://en.wikipedia.org/wiki/Sparse_dictionary_learning), o que tem várias aplicações dentro da área de Machine Learning, incluindo [melhorias na performance de algumas tarefas de classificação](https://arxiv.org/pdf/1312.5663.pdf). A imagem abaixo mostra uma rede com ativações mais esparsas devido à adição de um termo de regularização $\\mathcal{L}_{s}(z)$.\n",
    "\n",
    "![Sparse AE](https://www.dropbox.com/s/rs27590a80srntp/Sparse_AE_2.png?dl=1)\n",
    "\n",
    "Para mais informações sobre **Sparse Coding** (e também outros tópicos interessantes de Machine Learning), um bom material pode ser encontrado nos seguintes vídeos:\n",
    "*   https://www.youtube.com/watch?v=7a0_iEruGoM\n",
    "*   https://www.youtube.com/watch?v=L6qhzWWtqQs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56577fa",
   "metadata": {},
   "source": [
    "# AutoEncoder Variacional\n",
    "\n",
    "Idealmente codificações compactas de dados redundantes (i.e. imagens) deveriam produzir representações latentes que fossem independentes uma da outra num nível semântico. Ou seja, cada bin num feature map latente $z$ de um autoencoder deveria codificar o máximo de informação possível (i.e. linhas verticais que compõem um '1', '7' ou '9'; ou círculos que compõem um '6', '8' ou '0') para a reconstrução dos dígitos do MNIST, por exemplo. A [inferência variacional](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf) provém uma forma mais simples de computarmos o Maximum a Posteriori (MAP) de distribuições estatísticas complexas como as que estamos lidando.\n",
    "\n",
    "![VAE Features](https://www.dropbox.com/s/fkvdn69tkh7tm1p/vae_gaussian.png?dl=1)\n",
    "\n",
    "Se tivermos controle sobre representações latentes em $z$ que codificam features de algo nível semântico, podemos utilizar o Decoder de um AE para geração de novas amostras. Usando o Encoder de um AE tradicional, conseguimos partir do vetor de entrada $x$ e chegar no vetor latente $z \\sim q(z ∣ x)$. Porém, como não temos controle sobre a distribuição $q$, não é possível fazer o caminho inverso, ou seja, a partir de $z$ modelar $x \\sim p(x | z)$. Essa é a motivação para um Variational AutoEncoder (VAE).\n",
    "\n",
    "![VAE x->z](https://www.dropbox.com/s/o8daaskdrhfav7r/VAE_Enc.png?dl=1)\n",
    "\n",
    "![VAE z->x](https://www.dropbox.com/s/wqi8nsak84i11mi/VAE_Dec.png?dl=1)\n",
    "\n",
    "Para podermos ter um controle maior sobre distribuição de cada bin de $z$, adicionamos uma \"regularização\" $\\mathcal{L}_{KL}(\\mu, \\sigma)$ à loss de regressão $\\mathcal{L}_{r}(x, \\hat{x})$ de um AE tradicional. Percebe-se que $\\mu$ e $\\sigma$ devem codificar a média e o desvio padrão de distribuições gaussianas multivariadas, o que permite realizarmos uma amostragem dessa distribuição. Não podemos, porém, backpropagar de nós na nossa rede que realizem amostragem de uma distribuição. Portanto, precisamos do truque da reparametrização mostrado abaixo para backpropagarmos apenas por $\\mu$ e $\\sigma$, mas não por $\\epsilon$.\n",
    "\n",
    "![Reparametrization](https://jaan.io/images/reparametrization.png)\n",
    "\n",
    "Assim, a arquitetura final de um VAE segue o esquema a seguir composto no bottleneck por um vetor $\\mu$, um vetor $\\sigma$ e um vetor $\\epsilon$, que formam a representação latente $z = \\mu + \\sigma * \\epsilon$.\n",
    "\n",
    "![VAE training](https://www.dropbox.com/s/719vkfnfsobimmd/VAE_training.png?dl=1)\n",
    "\n",
    "A ideia é que cada gaussiana codifique uma característica de alto nível nos dados, permitindo que utilizemos o modelo generativo do VAE para, de fato, gerar amostras novas verossímeis no domínio dos dados de treino.\n",
    "\n",
    "![VAE gif](https://media.giphy.com/media/26ufgj5LH3YKO1Zlu/giphy.gif)\n",
    "\n",
    "Para entender mais sobre \"disentangled representations\", ler o paper original do [VAE](https://arxiv.org/abs/1312.6114), o [$\\beta$-VAE](https://openreview.net/references/pdf?id=Sy2fzU9gl) e o paper que propõe as [InfoGANs](https://arxiv.org/pdf/1606.03657.pdf):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832b25e",
   "metadata": {},
   "source": [
    "# Definindo as arquiteturas de $D$ e $G$\n",
    "\n",
    "\n",
    "Nesse primeiro notebook, vamos ver como um gerador bastante básico desempenha a função de gerar uma imagem. Nesse caso nosso gerador terá apenas camadas densas (*fully-connected*), recebendo como entrada um vetor de ruído gaussiano $z$ e dando como saída uma imagem de dimensões $(28 \\times 28)$. Nesse caso a arquitetura já estará implementada e é constituída de 3 camadas, em que:\n",
    "\n",
    "1. A primeira camada recebe como entrada o ruído $z$ de tamanho $100$ e retorna uma saída de tamanho $512$. Além disso temos um módulo de BatchNorm e ativação ReLU.\n",
    "1. A segunda camada recebe como entrada a saída da camanda anterior e também retorna uma saída de tamanho $512$. Além disso temos um módulo de BatchNorm e ativação ReLU.\n",
    "1. A terceira camada recebe como entrada a saída da camanda anterior e retorna uma saída de tamanho $784 = 1 \\times 28 \\times 28$. Depois disso temos uma ativação sigmóide.\n",
    "\n",
    "A sigmóide é usada porque nossas imagens retornadas do conjunto de dados estão normalizadas para o intervalo $[0, 1]$, através do `transforms.ToTensor()`. Portanto, nossas imagens geradas também precisam estar nessa mesma faixa.\n",
    "\n",
    "\n",
    "A arquitetura do discriminador será uma CNN padrão. Nesta GAN básica, essa arquitetura não tem nada de diferente das outras redes convolucionais que vimos, até porque o trabalho dela ainda é receber uma imagem como entrada e dar um *score* único na saída. A única particularidade é que nesse caso ao invés desse *score* ser a probabilidade da imagem ser de uma dada classe, é a probabilidade da imagem ser real. Portanto, podemos pensar que é como um problema de classificação binária, em que as classes das imagens são `real` e `falsa`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aeecc0",
   "metadata": {},
   "source": [
    "# Atividade Prática: Implementando uma GAN\n",
    "\n",
    "Nessa atividade implementaremos os principais elementos de uma GAN tradicional de acordo com os passos a seguir:\n",
    "\n",
    "1.   Defina dessa vez **dois** otimizadores, um para os parâmetros de $G$ e um para os parâmetros de $D$;\n",
    "1.   Defina [schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) de learning rate do tipo [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) para diminuir a Learning Rate a cada 5 epochs. Um scheduler deve atender a cada otimizador;\n",
    "1.   Definir o criterion da loss composta. Apesar da função de loss ser oposta entre $G$ e $D$, só precisamos definir o criterion uma vez. Em GANs tradicionais, usamos a BCE;\n",
    "1. Complemente a função *train()*.\n",
    "\n",
    "# Definindo a loss composta\n",
    "\n",
    "Como foi dito na parte da definição do discriminador, a tarefa do discriminador nas GANs pode ser vista como uma tarefa de classificação binária. Portanto, na definição dessa função de perda, podemos usar a mesma loss de entropia cruzada binária que usamos nesses problemas.\n",
    "\n",
    "# Criando funções para Treino e Teste\n",
    "\n",
    "O processo de treinamento das GANs é bastante diferente do treinamento de uma rede padrão para classificação. Porém, podemos \"separar\" o treinamento das GANs em duas partes, o que deixa o entendimento e a implementação mais fácil. Na primeira parte, consideramos que o Gerador é fixo, e atualizamos apenas o Discriminador; na segunda parte fazemos um processo análogo: consideramos que o Discriminador é fixo e atualizamos apenas o Gerador. Essas duas etapas podem ser detalhadas da seguinte forma:\n",
    "\n",
    "## Atualizando o discriminador\n",
    "\n",
    "Nessa etapa, consideramos que o Gerador é fixo, e portanto atualizamos apenas o Discriminador. Assim, essa etapa se torna ainda mais parecida de um treinamento de uma rede para classificação binária. Podemos pensar que as imagens que o Discriminador deve separar vêm de duas \"fontes de dados\" diferentes. As imagens reais vêm do conjunto de dados, e as imagens falsas vêm do Gerador. Portanto, precisamos treinar o Discriminador a classificar de qual dessas duas fontes de dados cada imagem vêm. Mais especificamente, nessa etapa seguimos os seguintes passos:\n",
    "\n",
    "1. Como as iamgens reais já são dadas pelo dataset, podemos primeiramente obter o *score* do Discriminador para essas imagens. Para isso, precisamos apenas passar as imagens reais pelo Discriminador.\n",
    "1. Obtemos a perda para as imagens reais. Queremos que o Discriminador classifique os dados reais como a classe `real`, que é representada pelos labels `y = 1`. Portanto, essa etapa consiste em calcular o erro de entropia cruzada entre os *scores* obtidos para as imagens reais e um tensor de 1's (que representa os labels das imagens reais). Essa perda calculada será uma das duas losses que usaremos para atualizar o Discriminador.\n",
    "1. Obtemos as imagens falsas: precisamos obter imagens falsas através do Gerador. Para isso usamos o ruído $z$ gaussiano que amostramos com `torch.rand`.\n",
    "1. Obtemos os *scores* do Discriminador para as imagens falsas. Essa etapa parece com a etapa 1, com a diferença que são imagens falsas.\n",
    "1. Obtemos a perda para as imagens falsas. Como queremos que o Discriminador classifique essas imagens como `falso`, então calculamos a perda considerando que os labels dessas imagens são todos 0's. Essa etapa parece com a etapa 2, com a exceção que os labels serão todos zeros nesse caso. Essa loss será a segunda usada para compor a perda final do Discriminador.\n",
    "1. Por fim, agora que já temos a loss para imagens reais e para as falsas, podemos obter a loss final simplesmente somando as duas. Assim, podemos calcular o backpropagation e a usar o otimizador do Discriminador para atualizar os pesos.\n",
    "\n",
    "## Atualizando o gerador\n",
    "\n",
    "Nessa etapa consideramos que o Discriminador é fixo, e portanto atualizamos apenas o Gerador. De um certo ponto de vista, podemos pensar que o Gerador é uma rede que faz o trabalho de geração de dados, e o Discriminador é uma função matemática $f(x)$ fixa, completamente diferenciável, que funciona como uma função de perda para essa nossa tarefa de \"geração de dados\". O que queremos para atualizar o Gerador é: dado as imagens criadas pelo Gerador, queremos que a saída dada pelo Discriminador nessas imagens seja o mais próximo da classe `real` possível, porque isso significa que o Gerador consegue enganar o Discriminador. Portanto, usamos a loss de entropia cruzada, mas usamos como labels das imagens **falsas** o valor `y = 1`, porque nessa etapa o Gerador que estamos atualizando.\n",
    "\n",
    "Seguimos os seguintes passos:\n",
    "\n",
    "1. Obtemos as imagens falsas, através do Gerador. Para isso usamos o ruído $z$ gaussiano que amostramos com `torch.rand`.\n",
    "1. Obtemos os *scores* do Discriminador para as imagens falsas.\n",
    "1. Obtemos a perda para as imagens falsas. Nesse caso, queremos que o Discriminador classifique essas imagens como `real` porque estamos atualizando **o Gerador**, então calculamos a perda considerando que os labels dessas imagens são todos 1's.\n",
    "1. Por fim, agora que já temos a loss para o Gerador (no caso do Gerador é penas uma), podemos calcular o backpropagation e a usar o otimizador do Gerador para atualizar os pesos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4a5df",
   "metadata": {},
   "source": [
    "# Treinamento Adversarial Convolucional\n",
    "\n",
    "Como desde o começo GANs foram pensadas para imagens primariamente, era de se esperar que convoluções fossem inseridas em algum momento. O artigo original das [GANs](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), inclusive, contém testes entre arquiteturas parcialmente convolucionais e compostas apenas por camadas FC:\n",
    "\n",
    "*   Resultados Fully Connected no CIFAR;\n",
    "![FC GAN](https://www.dropbox.com/s/dbem2z7jjzodoyb/gan_fc_goodfellow.png?dl=1)\n",
    "*   Resultados Convolutionais no CIFAR.\n",
    "![Convolutional GAN](https://www.dropbox.com/s/z9ihvqb4a53eqvb/gan_conv_goodfellow.png?dl=1)\n",
    "\n",
    "A arquitetura de $G$ lembra o Decoder de um VAE com camadas que partem de um vetor aleatório $z$ e geram uma amostra final sintética $x$. Já a arquitetura de $D$ lembra uma CNN tradicional para classificação de imagens, como a AlexNet, VGG, ResNet ou DenseNet que já vimos previamente no curso:\n",
    "\n",
    "*   Arquitetura de uma Generativa $G$;\n",
    "![Generator Architecture](https://www.dropbox.com/s/yf4d4sb1xcv8bma/GANs_Architecture_G.png?dl=1)\n",
    "\n",
    "*   Arquitetura de uma Discriminativa $D$.\n",
    "![Discriminator Architecture](https://www.dropbox.com/s/72s95njsuuag5m6/GANs_Architecture_D.png?dl=1)\n",
    "\n",
    "Por muito tempo, porém, foi proibitivo criar GAN convolucionais com muitas camadas, pois haviam problemas sérios de convergência e instabilidade no treinamento. O artigo das [Deep Convolutional GANs](https://arxiv.org/pdf/1511.06434.pdf) mitigou a maior parte desses problemas, propondo uma arquitetura padrão, e hoje em dia é possível treinar uma Convolutional GAN com diversas camadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9607f",
   "metadata": {},
   "source": [
    "# Atividade Prática: Implementando uma GAN\n",
    "\n",
    "Nessa atividade implementaremos os principais elementos de uma GAN tradicional de acordo com os passos a seguir:\n",
    "\n",
    "1.   Implemente a rede generativa $G$ que vai otimizar a distribuição $p(z | x)$. $G$ é composta de dois blocos sequenciais: a) o bloco *self.fc* que codifica $z$ para uma dimensionalidade maior que case com a entrada das convoluções transpostas; e b) o bloco *self.deconv* que conta com convoluções transpostas que realizam o upsampling aprendido nas imagens. Devem ser colocados dois blocos lineares dentro de *self.fc* (linear, batchnorm1d e relu) e dois blocos de convolução transposta (convtranspose2d, batchnorm2d, relu, convtranspose2d, sigmoid) em *self.deconv*;\n",
    "2.   Implemente a rede discriminativa $D$ que vai otimizar a probabilidade das imagens de entrada serem reais ou falsas. Essa rede será basicamnete uma CNN com arquitetura quase simétrica a $G$, composta por dois blocos sequenciais: a) *self.conv* composto por dois blocos convolucionais (conv2d, batchnorm2d, leakyrely, conv2d, sigmoid); e b) dois blocos lineares como em $G$, mas com uma ativação do último bloco sendo uma sigmoide e sem batchnorm1d;\n",
    "3.   Defina dessa vez **dois** otimizadores, um para os parâmetros de $G$ e um para os parâmetros de $D$;\n",
    "4.   Defina [schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) de learning rate do tipo [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) para diminuir a Learning Rate a cada 5 epochs. Um scheduler deve atender a cada otimizador;\n",
    "5.   Definir o criterion da loss composta. Apesar da função de loss ser oposta entre $G$ e $D$, só precisamos definir o criterion uma vez. Em GANs tradicionais, usamos a BCE;\n",
    "6. Complemente a função *train()*.\n",
    "\n",
    "PS. 1: em $G$ a saída da *self.fc* deve ter dimensões $(B, 128 \\times 7 \\times 7)$ e deve ser transformada usando a *.view()* na função *forward()* para $(B, 128, 7, 7)$ para entrar em *self.deconv*. A saída da *self.deconv* deve ter dimensões $(B, 1 \\times 28 \\times 28)$, o que são as dimensões de uma sample do MNIST.\n",
    "\n",
    "PS. 2: $D$ deve fazer o caminho inverso de $G$, saindo de *self.conv* com $(B, 128, 7, 7)$, linearizando as dimensões espaciais e canais usando a *.view()* e saindo de $self.fc$ com $(B, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d9404",
   "metadata": {},
   "source": [
    "Nesse notebook, vamos implementar uma função de perda proposta pela [LSGAN](https://arxiv.org/abs/1611.04076) (*Least Squares GAN*). O problema que esse trabalho ataca é aquele dos gradientes de baixa qualidade que a função de perda da Entropia Cruzada fornece em algumas condições específicas. Por isso, esse trabalho usa uma função de perda baseada no erro quadrático:\n",
    "\n",
    "![LSGAN loss](https://wiseodd.github.io/img/2017-03-02-least-squares-gan/03.png)\n",
    "\n",
    "# Definindo o Gerador $G$\n",
    "\n",
    "![Generator Architecture](https://www.dropbox.com/s/yf4d4sb1xcv8bma/GANs_Architecture_G.png?dl=1)\n",
    "\n",
    "\n",
    "# Definindo o Discriminador $D$\n",
    "\n",
    "![Discriminator Architecture](https://www.dropbox.com/s/72s95njsuuag5m6/GANs_Architecture_D.png?dl=1)\n",
    "\n",
    "# Definindo um Scheduler para os Learning Rates\n",
    "\n",
    "\n",
    "# Definindo a loss composta\n",
    "\n",
    "Nesse ponto é onde precisamos usar uma função de perda diferente da GAN padrão. Na LSGAN, a função de perda que usamos é a MSE (*mean squared error*). Nesse caso, os valores para os labels: 0 e 1 tem uma interpretação diferente. Para otimizar o Discriminador, quando avalia as imagens reais ele deve aproximar seu escore do valor 1, enquanto que ao avaliar imagens falsas seu escore deve se aproximar de 0. Para otimizar o Gerador, temos o contrário, queremos que quando o Discriminador avalie imagens falsas, seu escore se aproxime de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef48e4",
   "metadata": {},
   "source": [
    "# Conditional GANS\n",
    "\n",
    "Nesse notebook vamos implementar a GAN condicional (cGAN), que é uma modificação importante da GAN padrão que gerou várias formas diferentes delas serem usadas. Essa modificação consiste em incluir a informação do label no processo de geração. Com isso, o gerador é informado que ele deve não apenas gerar uma imaagem aleatória, mas também que essa imagem deve ser de determinada classe. Essa informação também é passada para o discriminador, e portanto ele tem o dever de verificar se aquela imagem é real sabendo que ela é de uma dada classe.\n",
    "\n",
    "Como o processo de geração fica condicionado nos labels, essa técnica ficou conhecida como GAN condicional.\n",
    "\n",
    "# Treinamento Adversarial Condicional\n",
    "\n",
    "Podemos inserir o conceito de classe na nossa GAN ao passar o rótulo *c* de cada amostra para tanto $G$ quanto $D$. Dessa forma, $G$ vai otimizar a distribuição $p(x | z, c)$ usando o rótulo para gerar amostras da classe correta.\n",
    "\n",
    "![CGANS](https://www.dropbox.com/s/gqtc5710dsrd4rh/GANs_Architecture_CGAN.png?dl=1)\n",
    "\n",
    "Fazemos isso ao adicionar one-hot encodings de $c$ para cada amostra que são concatenados ao batch na dimensão 1 (tanto em $G$ quanto em $D$). Esse tipo de rede é conhecido como uma Conditional GAN (CGAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8cf30",
   "metadata": {},
   "source": [
    "# DCGAN \n",
    "\n",
    "Nesse notebook, vamos explorar algumas modificações de GANs que melhoram a GAN original em alguns aspectos. Em duas dessas abordagens (DCGAN e WGAN-GP), o objetivo era melhorar a estabilidade do treinamento, e em uma delas (Pix2Pix) o objetivo era extender as GANs para uma tarefa ligeiramente diferente, de \"tradução\" de imagem para imagem.\n",
    "\n",
    "# DCGANs\n",
    "\n",
    "A primeira arquitetura que vamos ver é a [DCGAN](https://arxiv.org/abs/1511.06434), que foi um trabalho seminal de 2016 que trouxe alguns padrões de arquitetura e de treinamento para as GANs. Antes desse trabalho, a geração de imagens de resolução média ainda era um desafio e com essa abordagem se atraiu ainda mais interesse de pesquisadores para as GANs. A arquitetura do Gerador é a seguinte:\n",
    "\n",
    "![DCGAN Architecture](https://miro.medium.com/max/700/1*KvMnRfb76DponICrHIbSdg.png)\n",
    "\n",
    "\n",
    "\n",
    "A cartilha para treinamento das GANs proposta pelo artigo é a seguinte:\n",
    "\n",
    "![DCGAN guideline](https://i.imgur.com/08EVNUb.png)\n",
    "\n",
    "\n",
    "\n",
    "Imagens geradas ao longo do treinamneto das DCGANs:\n",
    "\n",
    "![DCGANs](https://github.com/eriklindernoren/PyTorch-GAN/raw/master/assets/dcgan.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26646700",
   "metadata": {},
   "source": [
    "# Pix2Pix\n",
    "\n",
    "O [Pix2Pix](https://phillipi.github.io/pix2pix/) trouxe para as GANs uma tarefa bastante diferente da GAN tradicional. A partir da GAN condicional, os autores desse trabalho observaram que podemos condicionar o Gerador usando (praticamente) qualquer informação que pode ser fornecida como entrada de uma rede neural. No Pix2Pix, então foi proposta a idéia de condicionar a geração em uma imagem, com o objetivo de gerar uma outra imagem. Por isso, esse processo ficou chamado de tradução imagem-para-imagem (*image-to-image translation*). No artigo original, eles exemplificam vários domínios onde essa arquitetura pode ser usada, e criaram uma ferramenta para que a comunidade em geral pudesse usar a arquitetura de várias formas diferentes. O treinamento dessa arquitetura é feita da seguinte forma:\n",
    "\n",
    "![Pix2Pix D and G](https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67)\n",
    "\n",
    "Exemplos de imagens geradas por essa arquitetura (no artigo original, resultados melhores são reportados):\n",
    "\n",
    "![Pix2Pix Examples](https://github.com/eriklindernoren/PyTorch-GAN/raw/master/assets/pix2pix.png)\n",
    "\n",
    "Fazendo alguns procedimentos necessários:\n",
    "\n",
    "- Download dos dados usados para esse experimento (Facades Dataset)\n",
    "- Migrando para o diretório (importado do github) que contém as implementações das arquiteturas\n",
    "\n",
    "Funções de perda. No caso dessa tarefa, queremos que o Gerador seja capaz de criar imagens tanto que pareçam reais, quanto que seguem a estrutura global da imagem real \"ground truth\". Portanto, temos duas funções de perda:\n",
    "\n",
    "- Uma loss padrão das GANs de separação entre imagens reais e falsas `MSELoss`.\n",
    "- Outra que penaliza o quanto a imagem gerada por G se difere da imagem real esperada `L1Loss`. Essa loss é dita \"pixelwise\" porque ela penaliza a diferença de valores de cada pixel da imagem gerada com a imagem original.\n",
    "\n",
    "A segunda loss é necessária porque, se ela não existisse, o Gerador não seria penalizado por gerar uma imagem que parece real mas que não segue a estrutura da imagem na qual estamos condicionando.\n",
    "\n",
    "Instanciando o Gerador e Discriminador. No caso desse modelo, estamos usando a implementação definida no arquivo `models.py` presente no github que importamos. Ao importar esse módulo acima temos acesso à classe que instancia o objeto. A arqutietura do Gerador é uma UNet, já que essa tarefa tem um objetivo parecido com o de segmentação semântica. A arquitetura do Discriminador também é semelhante ao Discriminador padrão, com exceção de que ele da como saída um \"mapa de escores\", ao invés de um escore único. Essa técnica ficou conhecida como PatchGAN, porque em cada localidade desse mapa de escores, o Discriminador está avaliando um patch (corte) da imagem de entrada localmente, ao invés da imagem inteira."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7aa3e",
   "metadata": {},
   "source": [
    "# WGAN-GP\n",
    "\n",
    "Uma outra modificação na GAN padrão que surgiu com o objetivo de melhorar a estabilidade do treinamento foi a [Wasserstein-GAN](https://arxiv.org/abs/1701.07875). Essa modificação consiste em alterar a função de perda da GAN. No caso da WGAN, a função de perda mede o quanto o discriminador consegue separar dados reais e falsos sem considerar um \"limite\" de separação. Enquanto o discriminador puder aumentar mais o \"gap\" entre o score para dados reais e falsos ele vai aumentar. A vantagem disso é que a sua derivada tem valores significativos mesmo quando o discriminador é muito bom em separar os dados. Portanto o gradiente para o Gerador sempre é informativo. \n",
    "\n",
    "Uma restrição descrita no artigo original da WGAN era que para que as vantagens teóricas que a função de perda de Wasserstein se concretizassem, o discriminador precisava ser uma função *1-Lipchitz contínua*. Isso significa que o gradiente do discriminador com relação à imagem de entrada deve ter norma menor que 1. Para assegurar essa restrição, o artigo original propunha *clipar* os pesos do discriminador em valores em um intervalo pequeno (como $[-0.01, 0.01]$).\n",
    "\n",
    "Outro trabalho que extendeu o trabalho da WGAN propõe uma outra forma de assegurar essa restrição que funciona melhor: ter na função de perda um termo que penaliza a norma do gradiente de D ser diferente de 1:\n",
    "\n",
    "![WGAN-GP loss](https://i.imgur.com/fjyTgFi.png)\n",
    "\n",
    "Como essa função de perda proposta tem uma penalização para a norma do gradiente, esse trabalho ficou conhecido como [WGAN-GP](https://arxiv.org/abs/1704.00028) (*Wasserstein GAN Gradient Penalty*).\n",
    "\n",
    "![WGAN-GP](https://github.com/eriklindernoren/PyTorch-GAN/raw/master/assets/wgan_gp.gif)\n",
    "\n",
    "Definindo a arquitetura do Gerador e do Discriminador. A princípio, a arquitetura dos modelos pode ser constituída de camadas convolucionais (ou convolucional transposta) como definimos nos modelos anteriores. Porém, nesse experimento vamos usar camadas *fully connected* tanto para o gerador quanto para o discriminador. Mesmo assim, como a função de perda da WGAN-GP é mais poderosa, o gerador é capaz de aprender a criar imagens que se parecem com os dados originais do MNIST em poucas épocas.\n",
    "\n",
    "Função para calcular o *gradient penalty*. Como o PyTorch já possui um módulo responsável pelos processamentos necessários para a diferenciação automática, podemos calcular essa penalização (e a sua derivada) através de funções do próprio [`autograd`](https://pytorch.org/docs/stable/autograd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a97997",
   "metadata": {},
   "source": [
    "Neural Transfer Using PyTorch\n",
    "=============================\n",
    "\n",
    "\n",
    "**Author**: `Alexis Jacq <https://alexis-jacq.github.io>`_\n",
    " \n",
    "**Edited by**: `Winston Herring <https://github.com/winston6>`_\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "This tutorial explains how to implement the `Neural-Style algorithm <https://arxiv.org/abs/1508.06576>`__\n",
    "developed by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge.\n",
    "Neural-Style, or Neural-Transfer, allows you to take an image and\n",
    "reproduce it with a new artistic style. The algorithm takes three images,\n",
    "an input image, a content-image, and a style-image, and changes the input \n",
    "to resemble the content of the content-image and the artistic style of the style-image.\n",
    "\n",
    " \n",
    ".. figure:: /_static/img/neural-style/neuralstyle.png\n",
    "   :alt: content1\n",
    "\n",
    "\n",
    "\n",
    "Underlying Principle\n",
    "--------------------\n",
    "\n",
    "The principle is simple: we define two distances, one for the content\n",
    "($D_C$) and one for the style ($D_S$). $D_C$ measures how different the content\n",
    "is between two images while $D_S$ measures how different the style is\n",
    "between two images. Then, we take a third image, the input, and\n",
    "transform it to minimize both its content-distance with the\n",
    "content-image and its style-distance with the style-image. Now we can\n",
    "import the necessary packages and begin the neural transfer.\n",
    "\n",
    "Importing Packages and Selecting a Device\n",
    "-----------------------------------------\n",
    "Below is a  list of the packages needed to implement the neural transfer.\n",
    "\n",
    "-  ``torch``, ``torch.nn``, ``numpy`` (indispensables packages for\n",
    "   neural networks with PyTorch)\n",
    "-  ``torch.optim`` (efficient gradient descents)\n",
    "-  ``PIL``, ``PIL.Image``, ``matplotlib.pyplot`` (load and display\n",
    "   images)\n",
    "-  ``torchvision.transforms`` (transform PIL images into tensors)\n",
    "-  ``torchvision.models`` (train or load pre-trained models)\n",
    "-  ``copy`` (to deep copy the models; system package)\n",
    "\n",
    "\n",
    "Next, we need to choose which device to run the network on and import the\n",
    "content and style images. Running the neural transfer algorithm on large\n",
    "images takes longer and will go much faster when running on a GPU. We can\n",
    "use ``torch.cuda.is_available()`` to detect if there is a GPU available.\n",
    "Next, we set the ``torch.device`` for use throughout the tutorial. Also the ``.to(device)``\n",
    "method is used to move tensors or modules to a desired device. \n",
    "\n",
    "\n",
    "Loading the Images\n",
    "------------------\n",
    "\n",
    "Now we will import the style and content images. The original PIL images have values between 0 and 255, but when\n",
    "transformed into torch tensors, their values are converted to be between\n",
    "0 and 1. The images also need to be resized to have the same dimensions.\n",
    "An important detail to note is that neural networks from the\n",
    "torch library are trained with tensor values ranging from 0 to 1. If you\n",
    "try to feed the networks with 0 to 255 tensor images, then the activated\n",
    "feature maps will be unable to sense the intended content and style.\n",
    "However, pre-trained networks from the Caffe library are trained with 0\n",
    "to 255 tensor images. \n",
    "\n",
    "\n",
    ".. Note::\n",
    "    Here are links to download the images required to run the tutorial:\n",
    "    `picasso.jpg <https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg>`__ and\n",
    "    `dancing.jpg <https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg>`__.\n",
    "    Download these two images and add them to a directory\n",
    "    with name ``images`` in your current working directory.\n",
    "\n",
    "\n",
    "Now, let's create a function that displays an image by reconverting a \n",
    "copy of it to PIL format and displaying the copy using \n",
    "``plt.imshow``. We will try displaying the content and style images \n",
    "to ensure they were imported correctly.\n",
    "\n",
    "\n",
    "Loss Functions\n",
    "--------------\n",
    "Content Loss\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "The content loss is a function that represents a weighted version of the\n",
    "content distance for an individual layer. The function takes the feature\n",
    "maps $F_{XL}$ of a layer $L$ in a network processing input $X$ and returns the\n",
    "weighted content distance $w_{CL}.D_C^L(X,C)$ between the image $X$ and the\n",
    "content image $C$. The feature maps of the content image($F_{CL}$) must be\n",
    "known by the function in order to calculate the content distance. We\n",
    "implement this function as a torch module with a constructor that takes\n",
    "$F_{CL}$ as an input. The distance $\\|F_{XL} - F_{CL}\\|^2$ is the mean square error\n",
    "between the two sets of feature maps, and can be computed using ``nn.MSELoss``.\n",
    "\n",
    "We will add this content loss module directly after the convolution\n",
    "layer(s) that are being used to compute the content distance. This way\n",
    "each time the network is fed an input image the content losses will be\n",
    "computed at the desired layers and because of auto grad, all the\n",
    "gradients will be computed. Now, in order to make the content loss layer\n",
    "transparent we must define a ``forward`` method that computes the content\n",
    "loss and then returns the layer’s input. The computed loss is saved as a\n",
    "parameter of the module.\n",
    "~~~~~~~~~~~~\n",
    ".. Note::\n",
    "   **Important detail**: although this module is named ``ContentLoss``, it\n",
    "   is not a true PyTorch Loss function. If you want to define your content\n",
    "   loss as a PyTorch Loss function, you have to create a PyTorch autograd function \n",
    "   to recompute/implement the gradient manually in the ``backward``\n",
    "   method.\n",
    "\n",
    "Style Loss\n",
    "~~~~~~~~~~\n",
    "\n",
    "The style loss module is implemented similarly to the content loss\n",
    "module. It will act as a transparent layer in a\n",
    "network that computes the style loss of that layer. In order to\n",
    "calculate the style loss, we need to compute the gram matrix $G_{XL}$. A gram\n",
    "matrix is the result of multiplying a given matrix by its transposed\n",
    "matrix. In this application the given matrix is a reshaped version of\n",
    "the feature maps $F_{XL}$ of a layer $L$. $F_{XL}$ is reshaped to form $\\hat{F}_{XL}$, a $K$\\ x\\ $N$\n",
    "matrix, where $K$ is the number of feature maps at layer $L$ and $N$ is the\n",
    "length of any vectorized feature map $F_{XL}^k$. For example, the first line\n",
    "of $\\hat{F}_{XL}$ corresponds to the first vectorized feature map $F_{XL}^1$.\n",
    "\n",
    "Finally, the gram matrix must be normalized by dividing each element by\n",
    "the total number of elements in the matrix. This normalization is to\n",
    "counteract the fact that $\\hat{F}_{XL}$ matrices with a large $N$ dimension yield\n",
    "larger values in the Gram matrix. These larger values will cause the\n",
    "first layers (before pooling layers) to have a larger impact during the\n",
    "gradient descent. Style features tend to be in the deeper layers of the\n",
    "network so this normalization step is crucial.\n",
    "~~~~~~~~~~\n",
    "\n",
    "Now the style loss module looks almost exactly like the content loss\n",
    "module. The style distance is also computed using the mean square\n",
    "error between $G_{XL}$ and $G_{SL}$.\n",
    "\n",
    "Importing the Model\n",
    "-------------------\n",
    "\n",
    "Now we need to import a pre-trained neural network. We will use a 19\n",
    "layer VGG network like the one used in the paper.\n",
    "\n",
    "PyTorch’s implementation of VGG is a module divided into two child\n",
    "``Sequential`` modules: ``features`` (containing convolution and pooling layers),\n",
    "and ``classifier`` (containing fully connected layers). We will use the\n",
    "``features`` module because we need the output of the individual\n",
    "convolution layers to measure content and style loss. Some layers have\n",
    "different behavior during training than evaluation, so we must set the\n",
    "network to evaluation mode using ``.eval()``.\n",
    "\n",
    "Additionally, VGG networks are trained on images with each channel\n",
    "normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "We will use them to normalize the image before sending it into the network.\n",
    "\n",
    "A ``Sequential`` module contains an ordered list of child modules. For\n",
    "instance, ``vgg19.features`` contains a sequence (Conv2d, ReLU, MaxPool2d,\n",
    "Conv2d, ReLU…) aligned in the right order of depth. We need to add our\n",
    "content loss and style loss layers immediately after the convolution\n",
    "layer they are detecting. To do this we must create a new ``Sequential``\n",
    "module that has content loss and style loss modules correctly inserted.\n",
    "\n",
    "Gradient Descent\n",
    "----------------\n",
    "\n",
    "As Leon Gatys, the author of the algorithm, suggested `here <https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-artistic-style/336/20?u=alexis-jacq>`__, we will use\n",
    "L-BFGS algorithm to run our gradient descent. Unlike training a network,\n",
    "we want to train the input image in order to minimise the content/style\n",
    "losses. We will create a PyTorch L-BFGS optimizer ``optim.LBFGS`` and pass\n",
    "our image to it as the tensor to optimize.\n",
    "\n",
    "Finally, we must define a function that performs the neural transfer. For\n",
    "each iteration of the networks, it is fed an updated input and computes\n",
    "new losses. We will run the ``backward`` methods of each loss module to\n",
    "dynamicaly compute their gradients. The optimizer requires a “closure”\n",
    "function, which reevaluates the module and returns the loss.\n",
    "\n",
    "We still have one final constraint to address. The network may try to\n",
    "optimize the input with values that exceed the 0 to 1 tensor range for\n",
    "the image. We can address this by correcting the input values to be\n",
    "between 0 to 1 each time the network is run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d2a25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7adcc28b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3606884f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aad12603",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c22980f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Resumo_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
