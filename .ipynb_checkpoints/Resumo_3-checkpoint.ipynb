{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb470a27",
   "metadata": {
    "id": "bb470a27"
   },
   "source": [
    "<a id=\"Topo\"></a><b><p style=\"text-align:center;font-size:24px\">Algoritmos de Aprendizado Profundo </p> </b> \n",
    "<p style=\"text-align:center;font-size:6px\"></p>\n",
    "<b><p style=\"text-align:center;font-size:16px\"> Resumo - Módulo 3 </p> </b>\n",
    "\n",
    "---\n",
    "\n",
    "<b><p style=\"text-align:center;font-size:24px\"> Índice </p> </b> \n",
    "\n",
    "**[I - Aprendizado não supervisionado](#Apns)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. Introdução](#Intro)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1.1. ](#)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.Métodos Clássicos](#Metodos)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.1. PCA](#PCA)**    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.2. Variantes](#Variantes)**    \n",
    "\n",
    "**[II - Autoencoders](#AutoEncoders)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. Encoder](#Encoders)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1.1. ](#)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2. Decoder](#Decoders)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[3. Denoising Autoencoders](#Denoising)**   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[4. Exemplos](#ExemplosAE)**   \n",
    "\n",
    "**[III - Variational Autoencoders](#VAE)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. ](#)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2. ](#)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2.1. ](#)**      \n",
    "\n",
    "\n",
    "**[IV - GANs](#GANs)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[1. Definições](#Def)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**[2. GANs mais recentes](#Recentes)**  \n",
    "\n",
    "\n",
    "**[Lista de exercícios 1](#Lista1)**  \n",
    "**[Lista de exercícios 2](#Lista2)**  \n",
    "**[Lista de exercícios 3](#Lista3)**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2f01c",
   "metadata": {},
   "source": [
    "https://www.amazon.com.br/Learning-Python-Second-Fran%C3%A7ois-Chollet/dp/1617296864/ref=asc_df_1617296864/?tag=googleshopp00-20&linkCode=df0&hvadid=379787788238&hvpos=&hvnetw=g&hvrand=8481696284822609082&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9101079&hvtargid=pla-1214669039719&psc=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc1cd35",
   "metadata": {
    "id": "2cc1cd35"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.ndimage.filters import convolve\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712e4da",
   "metadata": {},
   "source": [
    "# <a id=\"Apns\"></a> I - Aprendizado não-supervisionado \n",
    "\n",
    "Quando temos situações de aprendizado supervisionado, temos os rótulos, que são a resposta\n",
    "\n",
    "O foco da classificação supervisionada é entender como a variável resposta (y, ou label) responde coma s mudanças em g(x)\n",
    "\n",
    "\n",
    "Na classificação não-supervisionada, existe um grupo grande de variável.\n",
    "Não existe um conjunto de features às quais elas respondem.\n",
    "\n",
    "Queremos descobrir estruturas não-obvias nos dados.\n",
    "\n",
    "Tarefas:\n",
    "* Redução de dimensionalidade e visualização dos dados\n",
    "\n",
    "* Descoberta de clusters / grupos / classe -> Não existem rótulos para classe\n",
    "\n",
    "* Descoberta de estrutura (probabilistic graphical models, Bayesian networks)\n",
    "\n",
    "* Modelos generativos (GANs)\n",
    "\n",
    "<b><p style=\"text-align:right\"> [Retornar ao topo](#Topo) </p> </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920305dd",
   "metadata": {},
   "source": [
    "# <a id=\"Metodos\"></a> 2. Métodos Clássicos\n",
    "\n",
    "Alguns desafios de alta dimensão:\n",
    "    \n",
    "A maldição da dimensionalidade\n",
    "Dados de alta dimensão constumam viver na fronteira do espaço amostral\n",
    "Essa maldição não ocorre em dimensões menores\n",
    "DAdos aleatórios com disstribuição uniforme no quadrado [-1,1] x [-1,1]\n",
    "Dados num círculo  ocupam piR² = 81% do quadrado\n",
    "\n",
    "\n",
    "\n",
    "Quando aumentamos para 3 dimensões, uma esfera circunscrita no cubo de lado 2 passa a ocupar 51% do volume\n",
    "\n",
    "Aumentando a dimensão, temos que dados uniformes em $[-1, 1]^d$, temos que uma esfera S d-dimensional, centrada em (0,0,0,0...,0) e de raio 1\n",
    "\n",
    "$$\\mathbb{P}(Y\\in S) \\sim \\bigg(\\frac{1}{2}\\bigg)^d$$\n",
    "\n",
    "Esses cálculos acima assumem que os dados estão uniformemente distribuídos num hipercubo.\n",
    "\n",
    "Temos também a **bênção da não-uniformidade**, que significa que os dados às vezes estão concentrados em \"variedades diferenciáveis\" (differentiable manifolds), que são equivalentes a superfícies \n",
    "\n",
    "\n",
    "## <a id=\"PCA\"></a> 2.1 Principal Component Analysis (PCA)\n",
    "\n",
    "[Excelente explicação de PCA - em inglês](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)  \n",
    "\n",
    "A análise de componentes principais é uma das técnicas de redução de dimensionalidade mais usadas na estatística. O objetivo desta técnica é obter os eixos sobre os quais a variância de um conjunto de dados multivariado é máxima. Isso pode ser bem exemplificado pela imagem abaixo.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/lNHqt.gif\" alt=\"PCA. Fonte:stack_exchange\" width=\"800\"/>\n",
    "\n",
    "Observe que a variância do conjunto é máxima quando a reta se alinha às marcas rosas, e nesta posição ele é o equivalente ao **primeiro componente principal**. Esta direção da reta é dada pelo autovetor correspondente ao maior autovalor da matriz de covariância.\n",
    "\n",
    "Os autovalores são diretamente proporcionais ao percentual da variância do conjunto em uma determinada direção (autovetor correspondente). Por este motivo, os autovalores normalizados (divididos pela soma, para somarem 1) são geralmente tratados como os indicadores de percentual da covariância explicada.\n",
    "\n",
    "Por ter a capacidade de identificar as direções onde a variância do conjunto de dados é máxima, o PCA pode ser utilizado como um método de redução de dimensionalidade, pois uma vez identificados os $x$ primeiros componentes principais, podemos selecioná-los, eliminando o demais enquanto preservamos boa parte da variância nos dados. Esta técnica é útil quando trabalhamos com dados de alta dimensionalidade, ou que possuem correlações implícitas.\n",
    "\n",
    "No *scikit-learn*, a classe PCA possui o método pca.explained_variance_, que retorna os autovalores ordenados. O plot de componente x autovalor é chamado de Scree-plot. Ele pode ser muito útil quando desejamos realizar um agrupamento não supervisionado, e não sabemos ao certo quantas classes utilizar. Outra utilidade é no caso em que desejamos reduzir a dimensionalidade dos nossos dados, mas não temos certeza de quantos componentes principais utilizar. Esta técnica é conhecida como _elbow-method_. \n",
    "\n",
    "<b><p style=\"text-align:right\"> [Retornar ao topo](#Topo) </p> </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fe7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular autovetores e autovalores, a dicaé usar o numpy.linalg.eig\n",
    "# que calcula ambos ao mesmo tempo.\n",
    "# Para PCA, usar o scikit-learn\n",
    "\n",
    "# No código abaixo, pca é uma variável referente à classe PCA treinada no conjunto de dados\n",
    "componentes = np.arange(pca.n_components_) + 1 # Soma 1 só para começar de 1\n",
    "plt.plot(componentes, pca.explained_variance_, 'o-', color = 'tab:blue', linewidth=2)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Componente principal')\n",
    "plt.ylabel('Autovalor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a6b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef54123",
   "metadata": {},
   "source": [
    "## 2. Autoencoders\n",
    "\n",
    "Autoencoders = Redes neurais treinadas com o objetivo de copiar o seu input para o seu output.\n",
    "\n",
    "Isto é, a entrada é aproximadamente igual à saída\n",
    "\n",
    "A regularização do autoencoder é uma regularização um tanto diferente da tradicional. Ela não é aplicada diretamente no peso das redes, mas na saída intermediária (Z). Isso ajuda a fazer com que alguns dos pesos da rede sejam zero, reduzindo.\n",
    "\n",
    "Denoising autoencoders tem a ideia de evitar que uma rede muito complexa possa aprender a função identididade.\n",
    "\n",
    "\n",
    "Ruidos estruturados são úteis em situações onde o ruído esperado já é conhecido, a exemplo de marcas d'água.\n",
    "\n",
    "\n",
    "<b><p style=\"text-align:right\"> [Retornar ao topo](#Topo) </p> </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec5a06",
   "metadata": {},
   "source": [
    "https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d9ae1",
   "metadata": {},
   "source": [
    "Autoencoders para textos usam redes recorrentes. Será que dá para usar.\n",
    "\n",
    "Reproduzir slides do Steven Flores. A representação latente dos autoencoders pode ser utilizada para clusterização, porque teoricamente ele produzirá uma "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafcbcf",
   "metadata": {},
   "source": [
    "# RETIRADO DOS NOTEBOOKS DE AULA\n",
    "\n",
    "Todo AutoEncoder (AE) é composto de uma arquitetura Encoder-Decoder, assim como algumas redes para segmentação semântica que vimos na semana passada (i.e. [U-Nets](https://arxiv.org/pdf/1505.04597.pdf) ou [SegNets](https://arxiv.org/pdf/1511.00561.pdf)). AEs tradicionais não eram treinados usando backpropagation, mas sim com o método de **Stacking** de pares de camadas, caracterizando uma estratégia **gulosa** que ignorava interdependências entre pesos de diferentes camadas. O treinamento dessas redes era feito usando **Stacking** porque o algoritmo de backpropagation que vimos ao longo do curso ainda não era estável o bastante na época que os AEs surgiram na literatura, não conseguindo propagar os erros em redes com mais camadas escondidas. AEs atuais são treinados usando backpropagation assim como as outras arquiteturas modernas.\n",
    "\n",
    "![Linear AE](https://www.dropbox.com/s/vdfhfz6jldmidsj/Linear_AE.png?dl=1)\n",
    "\n",
    "O Encoder em um AE tradicional é composto de camadas lineares que diminuem a dimensionalidade dos dados progressivamente, criando uma região de bottleneck no meio da codificação.\n",
    "\n",
    "![Linear AE Encoder](https://www.dropbox.com/s/fyozex4grkim3ze/Linear_AE_Encoder.png?dl=1)\n",
    "\n",
    "Já as camadas do Decoder num AE recuperam gradualmente a dimensionalidade dos dados, reconstruíndo-o da forma mais fiel possível.\n",
    "\n",
    "![Linear AE Decoder](https://www.dropbox.com/s/t67b2z8bh68ei1q/Linear_AE_Decoder.png?dl=1)\n",
    "\n",
    "Ao se usar AEs Lineares em imagens, faz-se necessário linearizar os dados antes de enviá-los para as camadas Fully Connected da rede, lembrando sempre de recuperar as dimensões da imagem decodificada no fim da rede. Tanto a linearização quanto a recuperação da resolução espacial podem ser feitas usando o método *.view()* dos tensores.\n",
    "\n",
    "Percebe-se que os dados passam de uma dimensionalidade alta na entrada para uma dimensionalidade consideravelmente baixa no bottleneck da rede. Portanto AEs podem ser vistos como métodos de redução de dimensionalidade -- assim como o [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) ou o [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) -- porém treináveis para atuarem em um domínio específico (no nosso caso, em reconstrução dígitos). Assim como outros métodos de redução de dimensionalidade, a intuição dos AEs é que as informações importantes para a reconstrução dos dados sejam codificadas no bottleneck enquanto as informações não importantes sejam ignoradas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fe542",
   "metadata": {},
   "source": [
    "# Usando a representação de bottleneck como Deep Features\n",
    "\n",
    "A camada de bottleneck de um AE Linear ou Convolucional pode ser linearizada e utilizada como **Deep Features** que podem servir de entrada para um algoritmo que vá realizar uma tarefa qualquer de Machine Learning (i.e. clusterização, regressão, classificação, etc). Dessa forma, o Encoder da rede passa a ser um extrator de **Deep Features** não-supervisionado.\n",
    "\n",
    "![Conv AE Extractor](https://www.dropbox.com/s/11s5a5ynlu4g4wp/Conv_AE_Extractor.png?dl=1)\n",
    "\n",
    "Nota-se que o Encoder foi otimizado para minimizar a reconstrução dos dados -- e não a classificação dos dígitos, no caso do MNIST -- conseguindo resultados de classificação que não podem ser equiparados aos modelos treinados de forma supervisionada end-to-end, porém ainda melhores que a maioria dos **Handcrafted Features**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8bea1",
   "metadata": {},
   "source": [
    "# Denoising AutoEncoder\n",
    "\n",
    "Tanto AEs Lineares quanto Convolucionais podem ser adaptados para tarefas diferentes da de redução de dimensionalidade e reconstrução de imagens. Ao adicionar ruído artificial às imagens de entrada antes de passá-las para a forward, por exemplo, é possível treinar um AE para aprender a ignorar esse ruído e reconstruir as imagens originais, efetivamente realizando uma filtragem de ruído treinável. Em suma, é isso que um Denoising AutoEncoder (DAE) faz, como mostra o esquema abaixo.\n",
    "\n",
    "![Denoising AE](https://www.dropbox.com/s/mekbapirkdvwkhx/Denoising_AE.png?dl=1)\n",
    "\n",
    "É perceptível que ainda é preciso ter amostras limpas de ruído para treinar esse tipo de método, tornando-o limitado em alguns cenários. Porém, utilizando o conhecimento que se tem sobre as distribuições de ruídos em diferentes tipos de imagens (i.e. [gaussiano](https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise), [salt-and-pepper](https://en.wikipedia.org/wiki/Salt-and-pepper_noise), [ruído quântico](https://www.sciencedirect.com/topics/chemistry/quantum-noise), etc) e realizando algumas suposições sobre a natureza das imagens, é possível construir com pequenas alterações no AE tradicional um removedor de ruído específico para os dados de um certo domínio.\n",
    "\n",
    "O ruído salt-and-pepper é determinado pixel-a-pixel por uma distribuição de Bernoulli. Ou seja, dados $M$ pixels ${A_{0}, A_{1}, ..., A_{M-1}}$ de uma imagem $A$ e uma probabilidade $p$, cada pixel $A_{i}$ possui uma probabilidade $\\frac{p}{2}$ de ser setado para 0 e uma probabilidade $\\frac{p}{2}$ de ser setado para 1. No caso do ruído aditivo gaussiano, valores de uma imagem de ruído $B$ (tal que $B_{i} \\sim N(0, std)$) amostrada aleatoriamente são somados a todos os pixels da imagem $A$, resultando numa imagem $C = A + B$.\n",
    "\n",
    "Métodos mais modernos (i.e. [Noise2Noise](https://arxiv.org/pdf/1803.04189.pdf)) conseguem convergir sem a necessidade de dados não afetados pelos ruídos, tornando-os altamente aplicáveis em áreas como [imagens biomédicas](http://www.sprawls.org/ppmi2/NOISE/) ou [sensoriamento remoto](https://en.wikipedia.org/wiki/Synthetic-aperture_radar), as quais normalmente não possuem amostras \"limpas\" para treinamento de um DAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b4e4b",
   "metadata": {},
   "source": [
    "# AutoEncoder Esparso\n",
    "\n",
    "[Regularizações](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2) são componentes comuns em vários métodos da área de Machine Learning que podem servir como um viés para que o algoritmo dê preferência a soluções mais simples, potencialmente prevenindo overfitting no caso de modelos **overcomplete** e/ou **small data**. A loss de um Sparse AE (SAE) adiciona um termo de regularização $\\mathcal{L}_{s}$ à loss de regressão $\\mathcal{L}_{r}$ de um AE tradicional. Dessa forma, a loss total $\\mathcal{L}_{t}$ é dada por:\n",
    "\n",
    "$\\mathcal{L}_{t}(x, \\hat{x}, z) = \\mathcal{L}_{r}(x, \\hat{x}) + \\lambda_{s} \\mathcal{L}_{s}(z).$\n",
    "\n",
    "Um AE tradicional produz features com ativações consideravelmente densas dos inputs passados a ele, já que, como o objetivo principal é reconstrução, toda informação possível deve ser mantida nas representações latentes da rede.\n",
    "\n",
    "![Dense AE](https://www.dropbox.com/s/nfiix8cfk5g9wue/Sparse_AE_1.png?dl=1)\n",
    "\n",
    "Em contraponto, SAEs produzem representações esparsas dos dados que podem ser utilizadas para realizar [**Sparse Coding**](https://en.wikipedia.org/wiki/Sparse_dictionary_learning), o que tem várias aplicações dentro da área de Machine Learning, incluindo [melhorias na performance de algumas tarefas de classificação](https://arxiv.org/pdf/1312.5663.pdf). A imagem abaixo mostra uma rede com ativações mais esparsas devido à adição de um termo de regularização $\\mathcal{L}_{s}(z)$.\n",
    "\n",
    "![Sparse AE](https://www.dropbox.com/s/rs27590a80srntp/Sparse_AE_2.png?dl=1)\n",
    "\n",
    "Para mais informações sobre **Sparse Coding** (e também outros tópicos interessantes de Machine Learning), um bom material pode ser encontrado nos seguintes vídeos:\n",
    "*   https://www.youtube.com/watch?v=7a0_iEruGoM\n",
    "*   https://www.youtube.com/watch?v=L6qhzWWtqQs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcede40",
   "metadata": {},
   "source": [
    "# AutoEncoder Variacional\n",
    "\n",
    "Idealmente codificações compactas de dados redundantes (i.e. imagens) deveriam produzir representações latentes que fossem independentes uma da outra num nível semântico. Ou seja, cada bin num feature map latente $z$ de um autoencoder deveria codificar o máximo de informação possível (i.e. linhas verticais que compõem um '1', '7' ou '9'; ou círculos que compõem um '6', '8' ou '0') para a reconstrução dos dígitos do MNIST, por exemplo. A [inferência variacional](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf) provém uma forma mais simples de computarmos o Maximum a Posteriori (MAP) de distribuições estatísticas complexas como as que estamos lidando.\n",
    "\n",
    "![VAE Features](https://www.dropbox.com/s/fkvdn69tkh7tm1p/vae_gaussian.png?dl=1)\n",
    "\n",
    "Se tivermos controle sobre representações latentes em $z$ que codificam features de algo nível semântico, podemos utilizar o Decoder de um AE para geração de novas amostras. Usando o Encoder de um AE tradicional, conseguimos partir do vetor de entrada $x$ e chegar no vetor latente $z \\sim q(z ∣ x)$. Porém, como não temos controle sobre a distribuição $q$, não é possível fazer o caminho inverso, ou seja, a partir de $z$ modelar $x \\sim p(x | z)$. Essa é a motivação para um Variational AutoEncoder (VAE).\n",
    "\n",
    "![VAE x->z](https://www.dropbox.com/s/o8daaskdrhfav7r/VAE_Enc.png?dl=1)\n",
    "\n",
    "![VAE z->x](https://www.dropbox.com/s/wqi8nsak84i11mi/VAE_Dec.png?dl=1)\n",
    "\n",
    "Para podermos ter um controle maior sobre distribuição de cada bin de $z$, adicionamos uma \"regularização\" $\\mathcal{L}_{KL}(\\mu, \\sigma)$ à loss de regressão $\\mathcal{L}_{r}(x, \\hat{x})$ de um AE tradicional. Percebe-se que $\\mu$ e $\\sigma$ devem codificar a média e o desvio padrão de distribuições gaussianas multivariadas, o que permite realizarmos uma amostragem dessa distribuição. Não podemos, porém, backpropagar de nós na nossa rede que realizem amostragem de uma distribuição. Portanto, precisamos do truque da reparametrização mostrado abaixo para backpropagarmos apenas por $\\mu$ e $\\sigma$, mas não por $\\epsilon$.\n",
    "\n",
    "![Reparametrization](https://jaan.io/images/reparametrization.png)\n",
    "\n",
    "Assim, a arquitetura final de um VAE segue o esquema a seguir composto no bottleneck por um vetor $\\mu$, um vetor $\\sigma$ e um vetor $\\epsilon$, que formam a representação latente $z = \\mu + \\sigma * \\epsilon$.\n",
    "\n",
    "![VAE training](https://www.dropbox.com/s/719vkfnfsobimmd/VAE_training.png?dl=1)\n",
    "\n",
    "A ideia é que cada gaussiana codifique uma característica de alto nível nos dados, permitindo que utilizemos o modelo generativo do VAE para, de fato, gerar amostras novas verossímeis no domínio dos dados de treino.\n",
    "\n",
    "![VAE gif](https://media.giphy.com/media/26ufgj5LH3YKO1Zlu/giphy.gif)\n",
    "\n",
    "Para entender mais sobre \"disentangled representations\", ler o paper original do [VAE](https://arxiv.org/abs/1312.6114), o [$\\beta$-VAE](https://openreview.net/references/pdf?id=Sy2fzU9gl) e o paper que propõe as [InfoGANs](https://arxiv.org/pdf/1606.03657.pdf):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc5b48",
   "metadata": {},
   "source": [
    "# Definindo as arquiteturas de $D$ e $G$\n",
    "\n",
    "\n",
    "Nesse primeiro notebook, vamos ver como um gerador bastante básico desempenha a função de gerar uma imagem. Nesse caso nosso gerador terá apenas camadas densas (*fully-connected*), recebendo como entrada um vetor de ruído gaussiano $z$ e dando como saída uma imagem de dimensões $(28 \\times 28)$. Nesse caso a arquitetura já estará implementada e é constituída de 3 camadas, em que:\n",
    "\n",
    "1. A primeira camada recebe como entrada o ruído $z$ de tamanho $100$ e retorna uma saída de tamanho $512$. Além disso temos um módulo de BatchNorm e ativação ReLU.\n",
    "1. A segunda camada recebe como entrada a saída da camanda anterior e também retorna uma saída de tamanho $512$. Além disso temos um módulo de BatchNorm e ativação ReLU.\n",
    "1. A terceira camada recebe como entrada a saída da camanda anterior e retorna uma saída de tamanho $784 = 1 \\times 28 \\times 28$. Depois disso temos uma ativação sigmóide.\n",
    "\n",
    "A sigmóide é usada porque nossas imagens retornadas do conjunto de dados estão normalizadas para o intervalo $[0, 1]$, através do `transforms.ToTensor()`. Portanto, nossas imagens geradas também precisam estar nessa mesma faixa.\n",
    "\n",
    "\n",
    "A arquitetura do discriminador será uma CNN padrão. Nesta GAN básica, essa arquitetura não tem nada de diferente das outras redes convolucionais que vimos, até porque o trabalho dela ainda é receber uma imagem como entrada e dar um *score* único na saída. A única particularidade é que nesse caso ao invés desse *score* ser a probabilidade da imagem ser de uma dada classe, é a probabilidade da imagem ser real. Portanto, podemos pensar que é como um problema de classificação binária, em que as classes das imagens são `real` e `falsa`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46e3a3",
   "metadata": {},
   "source": [
    "# Atividade Prática: Implementando uma GAN\n",
    "\n",
    "Nessa atividade implementaremos os principais elementos de uma GAN tradicional de acordo com os passos a seguir:\n",
    "\n",
    "1.   Defina dessa vez **dois** otimizadores, um para os parâmetros de $G$ e um para os parâmetros de $D$;\n",
    "1.   Defina [schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) de learning rate do tipo [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) para diminuir a Learning Rate a cada 5 epochs. Um scheduler deve atender a cada otimizador;\n",
    "1.   Definir o criterion da loss composta. Apesar da função de loss ser oposta entre $G$ e $D$, só precisamos definir o criterion uma vez. Em GANs tradicionais, usamos a BCE;\n",
    "1. Complemente a função *train()*.\n",
    "\n",
    "# Definindo a loss composta\n",
    "\n",
    "Como foi dito na parte da definição do discriminador, a tarefa do discriminador nas GANs pode ser vista como uma tarefa de classificação binária. Portanto, na definição dessa função de perda, podemos usar a mesma loss de entropia cruzada binária que usamos nesses problemas.\n",
    "\n",
    "# Criando funções para Treino e Teste\n",
    "\n",
    "O processo de treinamento das GANs é bastante diferente do treinamento de uma rede padrão para classificação. Porém, podemos \"separar\" o treinamento das GANs em duas partes, o que deixa o entendimento e a implementação mais fácil. Na primeira parte, consideramos que o Gerador é fixo, e atualizamos apenas o Discriminador; na segunda parte fazemos um processo análogo: consideramos que o Discriminador é fixo e atualizamos apenas o Gerador. Essas duas etapas podem ser detalhadas da seguinte forma:\n",
    "\n",
    "## Atualizando o discriminador\n",
    "\n",
    "Nessa etapa, consideramos que o Gerador é fixo, e portanto atualizamos apenas o Discriminador. Assim, essa etapa se torna ainda mais parecida de um treinamento de uma rede para classificação binária. Podemos pensar que as imagens que o Discriminador deve separar vêm de duas \"fontes de dados\" diferentes. As imagens reais vêm do conjunto de dados, e as imagens falsas vêm do Gerador. Portanto, precisamos treinar o Discriminador a classificar de qual dessas duas fontes de dados cada imagem vêm. Mais especificamente, nessa etapa seguimos os seguintes passos:\n",
    "\n",
    "1. Como as iamgens reais já são dadas pelo dataset, podemos primeiramente obter o *score* do Discriminador para essas imagens. Para isso, precisamos apenas passar as imagens reais pelo Discriminador.\n",
    "1. Obtemos a perda para as imagens reais. Queremos que o Discriminador classifique os dados reais como a classe `real`, que é representada pelos labels `y = 1`. Portanto, essa etapa consiste em calcular o erro de entropia cruzada entre os *scores* obtidos para as imagens reais e um tensor de 1's (que representa os labels das imagens reais). Essa perda calculada será uma das duas losses que usaremos para atualizar o Discriminador.\n",
    "1. Obtemos as imagens falsas: precisamos obter imagens falsas através do Gerador. Para isso usamos o ruído $z$ gaussiano que amostramos com `torch.rand`.\n",
    "1. Obtemos os *scores* do Discriminador para as imagens falsas. Essa etapa parece com a etapa 1, com a diferença que são imagens falsas.\n",
    "1. Obtemos a perda para as imagens falsas. Como queremos que o Discriminador classifique essas imagens como `falso`, então calculamos a perda considerando que os labels dessas imagens são todos 0's. Essa etapa parece com a etapa 2, com a exceção que os labels serão todos zeros nesse caso. Essa loss será a segunda usada para compor a perda final do Discriminador.\n",
    "1. Por fim, agora que já temos a loss para imagens reais e para as falsas, podemos obter a loss final simplesmente somando as duas. Assim, podemos calcular o backpropagation e a usar o otimizador do Discriminador para atualizar os pesos.\n",
    "\n",
    "## Atualizando o gerador\n",
    "\n",
    "Nessa etapa consideramos que o Discriminador é fixo, e portanto atualizamos apenas o Gerador. De um certo ponto de vista, podemos pensar que o Gerador é uma rede que faz o trabalho de geração de dados, e o Discriminador é uma função matemática $f(x)$ fixa, completamente diferenciável, que funciona como uma função de perda para essa nossa tarefa de \"geração de dados\". O que queremos para atualizar o Gerador é: dado as imagens criadas pelo Gerador, queremos que a saída dada pelo Discriminador nessas imagens seja o mais próximo da classe `real` possível, porque isso significa que o Gerador consegue enganar o Discriminador. Portanto, usamos a loss de entropia cruzada, mas usamos como labels das imagens **falsas** o valor `y = 1`, porque nessa etapa o Gerador que estamos atualizando.\n",
    "\n",
    "Seguimos os seguintes passos:\n",
    "\n",
    "1. Obtemos as imagens falsas, através do Gerador. Para isso usamos o ruído $z$ gaussiano que amostramos com `torch.rand`.\n",
    "1. Obtemos os *scores* do Discriminador para as imagens falsas.\n",
    "1. Obtemos a perda para as imagens falsas. Nesse caso, queremos que o Discriminador classifique essas imagens como `real` porque estamos atualizando **o Gerador**, então calculamos a perda considerando que os labels dessas imagens são todos 1's.\n",
    "1. Por fim, agora que já temos a loss para o Gerador (no caso do Gerador é penas uma), podemos calcular o backpropagation e a usar o otimizador do Gerador para atualizar os pesos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850366f",
   "metadata": {},
   "source": [
    "# Treinamento Adversarial Convolucional\n",
    "\n",
    "Como desde o começo GANs foram pensadas para imagens primariamente, era de se esperar que convoluções fossem inseridas em algum momento. O artigo original das [GANs](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), inclusive, contém testes entre arquiteturas parcialmente convolucionais e compostas apenas por camadas FC:\n",
    "\n",
    "*   Resultados Fully Connected no CIFAR;\n",
    "![FC GAN](https://www.dropbox.com/s/dbem2z7jjzodoyb/gan_fc_goodfellow.png?dl=1)\n",
    "*   Resultados Convolutionais no CIFAR.\n",
    "![Convolutional GAN](https://www.dropbox.com/s/z9ihvqb4a53eqvb/gan_conv_goodfellow.png?dl=1)\n",
    "\n",
    "A arquitetura de $G$ lembra o Decoder de um VAE com camadas que partem de um vetor aleatório $z$ e geram uma amostra final sintética $x$. Já a arquitetura de $D$ lembra uma CNN tradicional para classificação de imagens, como a AlexNet, VGG, ResNet ou DenseNet que já vimos previamente no curso:\n",
    "\n",
    "*   Arquitetura de uma Generativa $G$;\n",
    "![Generator Architecture](https://www.dropbox.com/s/yf4d4sb1xcv8bma/GANs_Architecture_G.png?dl=1)\n",
    "\n",
    "*   Arquitetura de uma Discriminativa $D$.\n",
    "![Discriminator Architecture](https://www.dropbox.com/s/72s95njsuuag5m6/GANs_Architecture_D.png?dl=1)\n",
    "\n",
    "Por muito tempo, porém, foi proibitivo criar GAN convolucionais com muitas camadas, pois haviam problemas sérios de convergência e instabilidade no treinamento. O artigo das [Deep Convolutional GANs](https://arxiv.org/pdf/1511.06434.pdf) mitigou a maior parte desses problemas, propondo uma arquitetura padrão, e hoje em dia é possível treinar uma Convolutional GAN com diversas camadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc5259",
   "metadata": {},
   "source": [
    "# Atividade Prática: Implementando uma GAN\n",
    "\n",
    "Nessa atividade implementaremos os principais elementos de uma GAN tradicional de acordo com os passos a seguir:\n",
    "\n",
    "1.   Implemente a rede generativa $G$ que vai otimizar a distribuição $p(z | x)$. $G$ é composta de dois blocos sequenciais: a) o bloco *self.fc* que codifica $z$ para uma dimensionalidade maior que case com a entrada das convoluções transpostas; e b) o bloco *self.deconv* que conta com convoluções transpostas que realizam o upsampling aprendido nas imagens. Devem ser colocados dois blocos lineares dentro de *self.fc* (linear, batchnorm1d e relu) e dois blocos de convolução transposta (convtranspose2d, batchnorm2d, relu, convtranspose2d, sigmoid) em *self.deconv*;\n",
    "2.   Implemente a rede discriminativa $D$ que vai otimizar a probabilidade das imagens de entrada serem reais ou falsas. Essa rede será basicamnete uma CNN com arquitetura quase simétrica a $G$, composta por dois blocos sequenciais: a) *self.conv* composto por dois blocos convolucionais (conv2d, batchnorm2d, leakyrely, conv2d, sigmoid); e b) dois blocos lineares como em $G$, mas com uma ativação do último bloco sendo uma sigmoide e sem batchnorm1d;\n",
    "3.   Defina dessa vez **dois** otimizadores, um para os parâmetros de $G$ e um para os parâmetros de $D$;\n",
    "4.   Defina [schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) de learning rate do tipo [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) para diminuir a Learning Rate a cada 5 epochs. Um scheduler deve atender a cada otimizador;\n",
    "5.   Definir o criterion da loss composta. Apesar da função de loss ser oposta entre $G$ e $D$, só precisamos definir o criterion uma vez. Em GANs tradicionais, usamos a BCE;\n",
    "6. Complemente a função *train()*.\n",
    "\n",
    "PS. 1: em $G$ a saída da *self.fc* deve ter dimensões $(B, 128 \\times 7 \\times 7)$ e deve ser transformada usando a *.view()* na função *forward()* para $(B, 128, 7, 7)$ para entrar em *self.deconv*. A saída da *self.deconv* deve ter dimensões $(B, 1 \\times 28 \\times 28)$, o que são as dimensões de uma sample do MNIST.\n",
    "\n",
    "PS. 2: $D$ deve fazer o caminho inverso de $G$, saindo de *self.conv* com $(B, 128, 7, 7)$, linearizando as dimensões espaciais e canais usando a *.view()* e saindo de $self.fc$ com $(B, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313e743",
   "metadata": {},
   "source": [
    "Nesse notebook, vamos implementar uma função de perda proposta pela [LSGAN](https://arxiv.org/abs/1611.04076) (*Least Squares GAN*). O problema que esse trabalho ataca é aquele dos gradientes de baixa qualidade que a função de perda da Entropia Cruzada fornece em algumas condições específicas. Por isso, esse trabalho usa uma função de perda baseada no erro quadrático:\n",
    "\n",
    "![LSGAN loss](https://wiseodd.github.io/img/2017-03-02-least-squares-gan/03.png)\n",
    "\n",
    "# Definindo o Gerador $G$\n",
    "\n",
    "![Generator Architecture](https://www.dropbox.com/s/yf4d4sb1xcv8bma/GANs_Architecture_G.png?dl=1)\n",
    "\n",
    "\n",
    "# Definindo o Discriminador $D$\n",
    "\n",
    "![Discriminator Architecture](https://www.dropbox.com/s/72s95njsuuag5m6/GANs_Architecture_D.png?dl=1)\n",
    "\n",
    "# Definindo um Scheduler para os Learning Rates\n",
    "\n",
    "\n",
    "# Definindo a loss composta\n",
    "\n",
    "Nesse ponto é onde precisamos usar uma função de perda diferente da GAN padrão. Na LSGAN, a função de perda que usamos é a MSE (*mean squared error*). Nesse caso, os valores para os labels: 0 e 1 tem uma interpretação diferente. Para otimizar o Discriminador, quando avalia as imagens reais ele deve aproximar seu escore do valor 1, enquanto que ao avaliar imagens falsas seu escore deve se aproximar de 0. Para otimizar o Gerador, temos o contrário, queremos que quando o Discriminador avalie imagens falsas, seu escore se aproxime de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280c2af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6261689d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b980071",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26701ed5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "666f8023",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f140a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6af27213",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8669cc41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ef13e69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30de3574",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Resumo_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
